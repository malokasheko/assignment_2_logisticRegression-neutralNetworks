{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0eb3c05b",
      "metadata": {
        "id": "0eb3c05b"
      },
      "source": [
        "## Part C — Comprehensive Analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e7e0126",
      "metadata": {
        "id": "8e7e0126"
      },
      "source": [
        "Purpose: Hyperparameter analysis and model comparison (Logistic, Softmax, Neural Network) on MNIST. This notebook reuses code from Part A and Part B provided by the student.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2ef9496",
      "metadata": {
        "id": "b2ef9496"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "This notebook expects the DataLoaders train_loader, val_loader, test_loader (flattened 28x28 -> 784) to be defined in the environment. It also uses the provided model implementations (Logistic, Softmax, CustomFeedforwardNN) and training helpers.\n",
        "\n",
        "Run cells in order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "96eeaf5b",
      "metadata": {
        "id": "96eeaf5b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random\n",
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "import os , sys\n",
        "## uncomment this if you are running on vs code\n",
        "# sys.path.append(os.path.abspath(\"../\"))\n",
        "# from src.logisitc_manual import LogisticRegression\n",
        "# from src.softmax_manual import SoftmaxRegression\n",
        "# from src.nn_manual import *\n",
        "# from src.data_preprocessing import *\n",
        "\n",
        "\n",
        "# For reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "aa58f687",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "aa58f687",
        "outputId": "fa02f6cc-6fc1-4adf-f752-6a13218d08e4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3e52e7d1-c308-4c40-ae92-adba81a60ecf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3e52e7d1-c308-4c40-ae92-adba81a60ecf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving __init__.py to __init__ (2).py\n",
            "Saving data_preprocessing.py to data_preprocessing (2).py\n",
            "Saving logisitc_manual.py to logisitc_manual (2).py\n",
            "Saving nn_manual.py to nn_manual (2).py\n",
            "Saving softmax_manual.py to softmax_manual (2).py\n"
          ]
        }
      ],
      "source": [
        "#comment if you are on vs code\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from logisitc_manual import LogisticRegression\n",
        "from softmax_manual import SoftmaxRegression\n",
        "from data_preprocessing import *\n",
        "from nn_manual import *"
      ],
      "metadata": {
        "id": "mwO7d43Ly2Jk"
      },
      "id": "mwO7d43Ly2Jk",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6d5f51bc",
      "metadata": {
        "id": "6d5f51bc"
      },
      "source": [
        "# Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5703377",
      "metadata": {
        "id": "d5703377"
      },
      "source": [
        "### For Logisitc Regression (Class 0 vs 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "b1c13df0",
      "metadata": {
        "id": "b1c13df0"
      },
      "outputs": [],
      "source": [
        "binary_data = MNISTDataLoader(batch_size=32, binary=True, digits=(0, 1))\n",
        "train_loader_bin, val_loader_bin, test_loader_bin = binary_data.get_loaders()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8526ca55",
      "metadata": {
        "id": "8526ca55"
      },
      "source": [
        "## For Softmax Regression and NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "f67bf9f6",
      "metadata": {
        "id": "f67bf9f6"
      },
      "outputs": [],
      "source": [
        "multi_data = MNISTDataLoader(batch_size=32, binary=False)\n",
        "train_loader, val_loader, test_loader = multi_data.get_loaders()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68f00348",
      "metadata": {
        "id": "68f00348"
      },
      "source": [
        "## C1 — Hyperparameter Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67071756",
      "metadata": {
        "id": "67071756"
      },
      "source": [
        "### C1.1 Learning Rate Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28bb1377",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28bb1377",
        "outputId": "f143e48e-ab35-4499-de90-4d91eddf5231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== LR = 0.001 ===\n",
            "Epoch 1/20 Train Loss: 1.9895 Train Acc: 0.3966 Val Loss: 1.6273 Val Acc: 0.6332\n",
            "Epoch 2/20 Train Loss: 1.2881 Train Acc: 0.7206 Val Loss: 1.0034 Val Acc: 0.7883\n",
            "Epoch 3/20 Train Loss: 0.8453 Train Acc: 0.8091 Val Loss: 0.7190 Val Acc: 0.8302\n",
            "Epoch 4/20 Train Loss: 0.6492 Train Acc: 0.8409 Val Loss: 0.5865 Val Acc: 0.8540\n",
            "Epoch 5/20 Train Loss: 0.5487 Train Acc: 0.8600 Val Loss: 0.5111 Val Acc: 0.8673\n",
            "Epoch 6/20 Train Loss: 0.4875 Train Acc: 0.8716 Val Loss: 0.4625 Val Acc: 0.8787\n",
            "Epoch 7/20 Train Loss: 0.4460 Train Acc: 0.8808 Val Loss: 0.4296 Val Acc: 0.8851\n",
            "Epoch 8/20 Train Loss: 0.4159 Train Acc: 0.8879 Val Loss: 0.4031 Val Acc: 0.8918\n",
            "Epoch 9/20 Train Loss: 0.3929 Train Acc: 0.8928 Val Loss: 0.3826 Val Acc: 0.8953\n",
            "Epoch 10/20 Train Loss: 0.3747 Train Acc: 0.8972 Val Loss: 0.3671 Val Acc: 0.8995\n",
            "Epoch 11/20 Train Loss: 0.3596 Train Acc: 0.9008 Val Loss: 0.3539 Val Acc: 0.9017\n",
            "Epoch 12/20 Train Loss: 0.3469 Train Acc: 0.9035 Val Loss: 0.3420 Val Acc: 0.9037\n",
            "Epoch 13/20 Train Loss: 0.3360 Train Acc: 0.9057 Val Loss: 0.3320 Val Acc: 0.9069\n",
            "Epoch 14/20 Train Loss: 0.3263 Train Acc: 0.9077 Val Loss: 0.3242 Val Acc: 0.9090\n",
            "Epoch 15/20 Train Loss: 0.3176 Train Acc: 0.9107 Val Loss: 0.3168 Val Acc: 0.9110\n",
            "Epoch 16/20 Train Loss: 0.3100 Train Acc: 0.9127 Val Loss: 0.3092 Val Acc: 0.9124\n",
            "Epoch 17/20 Train Loss: 0.3029 Train Acc: 0.9147 Val Loss: 0.3029 Val Acc: 0.9144\n",
            "Epoch 18/20 Train Loss: 0.2964 Train Acc: 0.9166 Val Loss: 0.2976 Val Acc: 0.9152\n",
            "Epoch 19/20 Train Loss: 0.2904 Train Acc: 0.9181 Val Loss: 0.2913 Val Acc: 0.9174\n",
            "Epoch 20/20 Train Loss: 0.2847 Train Acc: 0.9197 Val Loss: 0.2868 Val Acc: 0.9189\n",
            "\n",
            "=== LR = 0.01 ===\n",
            "Epoch 1/20 Train Loss: 0.7803 Train Acc: 0.7938 Val Loss: 0.3768 Val Acc: 0.8964\n",
            "Epoch 2/20 Train Loss: 0.3294 Train Acc: 0.9072 Val Loss: 0.2995 Val Acc: 0.9115\n",
            "Epoch 3/20 Train Loss: 0.2710 Train Acc: 0.9225 Val Loss: 0.2529 Val Acc: 0.9298\n",
            "Epoch 4/20 Train Loss: 0.2356 Train Acc: 0.9315 Val Loss: 0.2300 Val Acc: 0.9339\n",
            "Epoch 5/20 Train Loss: 0.2103 Train Acc: 0.9398 Val Loss: 0.2117 Val Acc: 0.9382\n",
            "Epoch 6/20 Train Loss: 0.1901 Train Acc: 0.9452 Val Loss: 0.1931 Val Acc: 0.9435\n",
            "Epoch 7/20 Train Loss: 0.1736 Train Acc: 0.9500 Val Loss: 0.1819 Val Acc: 0.9459\n",
            "Epoch 8/20 Train Loss: 0.1601 Train Acc: 0.9547 Val Loss: 0.1728 Val Acc: 0.9491\n",
            "Epoch 9/20 Train Loss: 0.1484 Train Acc: 0.9579 Val Loss: 0.1636 Val Acc: 0.9523\n",
            "Epoch 10/20 Train Loss: 0.1382 Train Acc: 0.9605 Val Loss: 0.1522 Val Acc: 0.9561\n",
            "Epoch 11/20 Train Loss: 0.1291 Train Acc: 0.9635 Val Loss: 0.1437 Val Acc: 0.9574\n",
            "Epoch 12/20 Train Loss: 0.1204 Train Acc: 0.9653 Val Loss: 0.1376 Val Acc: 0.9589\n",
            "Epoch 13/20 Train Loss: 0.1133 Train Acc: 0.9672 Val Loss: 0.1328 Val Acc: 0.9609\n",
            "Epoch 14/20 Train Loss: 0.1065 Train Acc: 0.9698 Val Loss: 0.1308 Val Acc: 0.9596\n",
            "Epoch 15/20 Train Loss: 0.1008 Train Acc: 0.9711 Val Loss: 0.1292 Val Acc: 0.9604\n",
            "Epoch 16/20 Train Loss: 0.0950 Train Acc: 0.9735 Val Loss: 0.1251 Val Acc: 0.9626\n",
            "Epoch 17/20 Train Loss: 0.0901 Train Acc: 0.9748 Val Loss: 0.1177 Val Acc: 0.9654\n",
            "Epoch 18/20 Train Loss: 0.0853 Train Acc: 0.9762 Val Loss: 0.1156 Val Acc: 0.9656\n",
            "Epoch 19/20 Train Loss: 0.0809 Train Acc: 0.9772 Val Loss: 0.1128 Val Acc: 0.9660\n",
            "Epoch 20/20 Train Loss: 0.0767 Train Acc: 0.9787 Val Loss: 0.1124 Val Acc: 0.9657\n",
            "\n",
            "=== LR = 0.1 ===\n",
            "Epoch 1/20 Train Loss: 0.3267 Train Acc: 0.9027 Val Loss: 0.1669 Val Acc: 0.9485\n",
            "Epoch 2/20 Train Loss: 0.1386 Train Acc: 0.9583 Val Loss: 0.1238 Val Acc: 0.9628\n",
            "Epoch 3/20 Train Loss: 0.0980 Train Acc: 0.9701 Val Loss: 0.1134 Val Acc: 0.9639\n",
            "Epoch 4/20 Train Loss: 0.0740 Train Acc: 0.9771 Val Loss: 0.1018 Val Acc: 0.9681\n",
            "Epoch 5/20 Train Loss: 0.0568 Train Acc: 0.9829 Val Loss: 0.1273 Val Acc: 0.9615\n",
            "Epoch 6/20 Train Loss: 0.0447 Train Acc: 0.9863 Val Loss: 0.0971 Val Acc: 0.9699\n",
            "Epoch 7/20 Train Loss: 0.0339 Train Acc: 0.9895 Val Loss: 0.0908 Val Acc: 0.9727\n",
            "Epoch 8/20 Train Loss: 0.0262 Train Acc: 0.9922 Val Loss: 0.0978 Val Acc: 0.9707\n",
            "Epoch 9/20 Train Loss: 0.0204 Train Acc: 0.9943 Val Loss: 0.0875 Val Acc: 0.9755\n",
            "Epoch 10/20 Train Loss: 0.0166 Train Acc: 0.9952 Val Loss: 0.0928 Val Acc: 0.9743\n",
            "Epoch 11/20 Train Loss: 0.0120 Train Acc: 0.9968 Val Loss: 0.0910 Val Acc: 0.9751\n",
            "Epoch 12/20 Train Loss: 0.0093 Train Acc: 0.9982 Val Loss: 0.0937 Val Acc: 0.9753\n",
            "Epoch 13/20 Train Loss: 0.0053 Train Acc: 0.9993 Val Loss: 0.0924 Val Acc: 0.9768\n",
            "Epoch 14/20 Train Loss: 0.0037 Train Acc: 0.9997 Val Loss: 0.0935 Val Acc: 0.9763\n",
            "Epoch 15/20 Train Loss: 0.0027 Train Acc: 0.9998 Val Loss: 0.0914 Val Acc: 0.9769\n",
            "Epoch 16/20 Train Loss: 0.0018 Train Acc: 0.9999 Val Loss: 0.0991 Val Acc: 0.9766\n",
            "Epoch 17/20 Train Loss: 0.0015 Train Acc: 1.0000 Val Loss: 0.0959 Val Acc: 0.9779\n"
          ]
        }
      ],
      "source": [
        "learning_rates = [0.001, 0.01, 0.1, 1.0]\n",
        "num_epochs = 20\n",
        "batch_size = 64\n",
        "results_lr = []\n",
        "for lr in learning_rates:\n",
        "    print(f\"\\n=== LR = {lr} ===\")\n",
        "    # ensure we use DataLoaders with the chosen batch size\n",
        "    # If your original train_loader has different batch size, recreate it from tensors if available\n",
        "    # Here we assume train_loader and val_loader accept any batch sizes; if not, recreate from TensorDataset.\n",
        "    model = CustomFeedforwardNN()\n",
        "    tl, vl, ta, va = train_model_once(model, train_loader, val_loader, epochs=num_epochs, learning_rate=lr)\n",
        "    results_lr.append({'lr': lr, 'train_losses': tl, 'val_losses': vl, 'train_acc': ta, 'val_acc': va})\n",
        "\n",
        "# Plot validation loss comparison\n",
        "plt.figure(figsize=(8,5))\n",
        "for res in results_lr:\n",
        "    plt.plot(res['val_losses'], label=f\"lr={res['lr']}\")\n",
        "plt.xlabel('Epoch'); plt.ylabel('Val Loss'); plt.title('Validation Loss for different learning rates'); plt.legend(); plt.grid(True); plt.show()\n",
        "\n",
        "# Summarize final val accuracy\n",
        "pd.DataFrame([{'lr': r['lr'], 'final_val_acc': r['val_acc'][-1]} for r in results_lr])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44ac1218",
      "metadata": {
        "id": "44ac1218"
      },
      "source": [
        "### C1.2 Batch Size Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "924d009d",
      "metadata": {
        "id": "924d009d"
      },
      "outputs": [],
      "source": [
        "batch_sizes = [16, 32, 64, 128]\n",
        "num_epochs = 20\n",
        "results_bs = []\n",
        "# If tensors for X_train_flat etc exist, recreate DataLoaders with different batch sizes\n",
        "try:\n",
        "    X_train_flat\n",
        "    recreate_loaders = True\n",
        "except NameError:\n",
        "    recreate_loaders = False\n",
        "\n",
        "for bs in batch_sizes:\n",
        "    print(f\"\\n=== Batch size = {bs} ===\")\n",
        "    if recreate_loaders:\n",
        "        train_loader_bs = DataLoader(TensorDataset(X_train_flat, y_train), batch_size=bs, shuffle=True)\n",
        "        val_loader_bs = DataLoader(TensorDataset(X_val_flat, y_val), batch_size=bs, shuffle=False)\n",
        "    else:\n",
        "        train_loader_bs = train_loader; val_loader_bs = val_loader\n",
        "    model = CustomFeedforwardNN()\n",
        "    start = time.time()\n",
        "    tl, vl, ta, va = train_model_once(model, train_loader_bs, val_loader_bs, epochs=num_epochs, learning_rate=0.01)\n",
        "    duration = time.time() - start\n",
        "    results_bs.append({'bs': bs, 'train_losses': tl, 'val_losses': vl, 'train_acc': ta, 'val_acc': va, 'time_s': duration})\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "for res in results_bs:\n",
        "    plt.plot(res['val_losses'], label=f\"bs={res['bs']}\")\n",
        "plt.xlabel('Epoch'); plt.ylabel('Val Loss'); plt.title('Validation Loss for different batch sizes'); plt.legend(); plt.grid(True); plt.show()\n",
        "\n",
        "pd.DataFrame([{'batch_size': r['bs'], 'final_val_acc': r['val_acc'][-1], 'time_s': r['time_s']} for r in results_bs])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e94df81a",
      "metadata": {
        "id": "e94df81a"
      },
      "source": [
        "### C1.3 Architecture Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c14257d",
      "metadata": {
        "id": "9c14257d"
      },
      "outputs": [],
      "source": [
        "def make_ffnn(hidden_sizes):\n",
        "    # hidden_sizes: list of ints\n",
        "    if len(hidden_sizes) == 1:\n",
        "        return CustomFeedforwardNN(input_size=784, hidden1_size=hidden_sizes[0], hidden2_size=64, output_size=10)\n",
        "    elif len(hidden_sizes) == 2:\n",
        "        return CustomFeedforwardNN(input_size=784, hidden1_size=hidden_sizes[0], hidden2_size=hidden_sizes[1], output_size=10)\n",
        "    else:\n",
        "        # for >2 layers create dynamic module\n",
        "        class FFN_dynamic(nn.Module):\n",
        "            def __init__(self, input_dim=784, hidden_sizes=[128,64], output_dim=10):\n",
        "                super().__init__()\n",
        "                layers = []\n",
        "                prev = input_dim\n",
        "                for h in hidden_sizes:\n",
        "                    layers.append(nn.Linear(prev, h))\n",
        "                    layers.append(nn.ReLU())\n",
        "                    prev = h\n",
        "                layers.append(nn.Linear(prev, output_dim))\n",
        "                self.net = nn.Sequential(*layers)\n",
        "                for m in self.net:\n",
        "                    if isinstance(m, nn.Linear):\n",
        "                        nn.init.xavier_uniform_(m.weight); nn.init.zeros_(m.bias)\n",
        "            def forward(self, x):\n",
        "                return self.net(x)\n",
        "        return FFN_dynamic(input_dim=784, hidden_sizes=hidden_sizes, output_dim=10)\n",
        "\n",
        "layers_options = [ [64,64], [128,64], [256,128,64], [512,256,128,64] ]\n",
        "results_arch = []\n",
        "for hidden in layers_options:\n",
        "    print(f\"\\n=== Arch: {hidden} ===\")\n",
        "    model = make_ffnn(hidden)\n",
        "    tl, vl, ta, va = train_model_once(model, train_loader, val_loader, epochs=20, learning_rate=0.01)\n",
        "    results_arch.append({'hidden': hidden, 'train_acc': ta[-1], 'val_acc': va[-1], 'train_losses': tl, 'val_losses': vl})\n",
        "\n",
        "pd.DataFrame([{'architecture': str(r['hidden']), 'train_acc': r['train_acc'], 'val_acc': r['val_acc']} for r in results_arch])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1e9ebfc",
      "metadata": {
        "id": "b1e9ebfc"
      },
      "source": [
        "## C2 — Model Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08a17133",
      "metadata": {
        "id": "08a17133"
      },
      "outputs": [],
      "source": [
        "# Softmax Regression (multiclass)\n",
        "soft_model = SoftmaxRegression(input_dim=784, num_classes=10, learning_rate=0.1,max_epochs=50)\n",
        "soft_model.fit(train_loader, val_loader)\n",
        "val_loss_soft, val_acc_soft = soft_model.evaluate(val_loader)\n",
        "print(\"Softmax val acc:\", val_acc_soft)\n",
        "\n",
        "# Neural network: choose best from architecture experiments (example pick index 1)\n",
        "best_hidden = results_arch[1]['hidden'] if len(results_arch)>1 else [128,64]\n",
        "best_model = make_ffnn(best_hidden)\n",
        "start = time.time()\n",
        "tl, vl, ta, va = train_model_once(best_model, train_loader, val_loader, epochs=30, learning_rate=0.01)\n",
        "nn_time = time.time() - start\n",
        "# Evaluate on test set utility\n",
        "\n",
        "def evaluate_nn_on_loader(model, loader):\n",
        "    model.to(device); model.eval()\n",
        "    correct = 0; total = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            y_true.extend(y_batch.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "            correct += (preds.cpu() == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "    return correct/total, np.array(y_true), np.array(y_pred)\n",
        "\n",
        "# Evaluate Softmax on test\n",
        "soft_test_loss, soft_test_acc = soft_model.evaluate(test_loader)\n",
        "print('Softmax test acc:', soft_test_acc)\n",
        "\n",
        "# Evaluate NN on test\n",
        "nn_test_acc, y_true_nn, y_pred_nn = evaluate_nn_on_loader(best_model, test_loader)\n",
        "print('NN test acc:', nn_test_acc)\n",
        "\n",
        "# Logistic: if binary problem, train logistic regression\n",
        "# If you want to compare on full 10-class, logistic is not directly used (would require one-vs-rest); we keep logistic for binary experiments.\n",
        "\n",
        "# Summary table\n",
        "comparison = pd.DataFrame([\n",
        "    {'Model': 'Softmax Regression', 'Test Accuracy': float(soft_test_acc), 'Training Time (s)': float('nan')},\n",
        "    {'Model': f'Neural Network {best_hidden}', 'Test Accuracy': float(nn_test_acc), 'Training Time (s)': nn_time}\n",
        "])\n",
        "comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "577e36a8",
      "metadata": {
        "id": "577e36a8"
      },
      "source": [
        "## C2.1 Confusion Matrix & Misclassified Examples (Best NN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfdee0c1",
      "metadata": {
        "id": "cfdee0c1"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_true_nn, y_pred_nn)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "disp.plot(ax=ax)\n",
        "plt.title('Confusion Matrix - Best NN')\n",
        "plt.show()\n",
        "\n",
        "# Show some misclassified examples\n",
        "mis_idx = np.where(y_true_nn != y_pred_nn)[0]\n",
        "print('Total misclassified:', len(mis_idx))\n",
        "\n",
        "# Grab images from test dataset (original test dataset should be available as X_test / y_test or test_loader.dataset)\n",
        "# Assuming test_loader.dataset is a TensorDataset of flattened images\n",
        "\n",
        "dataset_for_vis = test_loader.dataset\n",
        "fig, axes = plt.subplots(2,5, figsize=(12,5))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    if i >= len(mis_idx): break\n",
        "    idx = mis_idx[i]\n",
        "    # if dataset stores flattened images, reshape\n",
        "    img_flat, true_label = dataset_for_vis[idx]\n",
        "    if img_flat.ndim == 1:\n",
        "        img = img_flat.reshape(28,28)\n",
        "    else:\n",
        "        img = img_flat.squeeze()\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    ax.set_title(f\"Pred: {y_pred_nn[idx]} / True: {y_true_nn[idx]}\")\n",
        "    ax.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5363bacc",
      "metadata": {
        "id": "5363bacc"
      },
      "source": [
        "## C3 — Final Evaluation & Retraining Best Model on Train+Val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c29a730",
      "metadata": {
        "id": "9c29a730"
      },
      "outputs": [],
      "source": [
        "# Combine train + val into one dataset and retrain best model\n",
        "try:\n",
        "    X_train_flat; X_val_flat; y_train; y_val\n",
        "    X_train_comb = torch.cat([X_train_flat, X_val_flat], dim=0)\n",
        "    y_train_comb = torch.cat([y_train, y_val], dim=0)\n",
        "    combined_loader = DataLoader(TensorDataset(X_train_comb, y_train_comb), batch_size=64, shuffle=True)\n",
        "    final_model = make_ffnn(best_hidden)\n",
        "    # train\n",
        "    train_model_once(final_model, combined_loader, test_loader, epochs=30, learning_rate=0.01)\n",
        "    final_acc, y_true_final, y_pred_final = evaluate_nn_on_loader(final_model, test_loader)\n",
        "    print('Final test acc after retraining on train+val:', final_acc)\n",
        "except NameError:\n",
        "    print('Train/Val tensors not found in workspace; skip final retrain step or recreate tensors.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb02b0ab",
      "metadata": {
        "id": "eb02b0ab"
      },
      "source": [
        "## Save results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "194a46d9",
      "metadata": {
        "id": "194a46d9"
      },
      "outputs": [],
      "source": [
        "# Save best model weights (optional)\n",
        "try:\n",
        "    torch.save(final_model.state_dict(), 'best_model_final.pt')\n",
        "    print('Saved best_model_final.pt')\n",
        "except NameError:\n",
        "    print('Final model not trained — no file saved')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d06c7a2c",
      "metadata": {
        "id": "d06c7a2c"
      },
      "source": [
        "## End of Notebook"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}